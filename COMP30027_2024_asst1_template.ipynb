{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenryRoutson/Machine-learning-A1/blob/main/COMP30027_2024_asst1_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erc1R1AZ5SKs"
      },
      "source": [
        "###### ### The University of Melbourne, School of Computing and Information Systems\n",
        "# COMP30027 Machine Learning, 2024 Semester 1\n",
        "\n",
        "## Assignment 1: Wine quality classification with K-NN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP2wSH9J5SKt"
      },
      "source": [
        "**Student ID(s):**     `1307261`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8F1bb8Y5SKt"
      },
      "source": [
        "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
        "\n",
        "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
        "\n",
        "**Adding proper comments to your code is MANDATORY. **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%pip3` not found.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Hello, \n",
        "If you have any issues with the ipynb,\n",
        "try running main.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import random\n",
        "import math\n",
        "from typing import Callable\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################################################################\n",
        "# Constants\n",
        "\n",
        "TEST_PATH = 'COMP30027_2024_asst1_data/winequality-test.csv'\n",
        "TRAIN_PATH = 'COMP30027_2024_asst1_data/winequality-train.csv'\n",
        "\n",
        "TRAINING_DATA = np.loadtxt(TRAIN_PATH, delimiter=\",\", dtype=float, skiprows=1)\n",
        "\n",
        "TESTING_DATA  = np.loadtxt(TEST_PATH,  delimiter=\",\", dtype=float, skiprows=1)\n",
        "\n",
        "LOW_QUALITY_COLOR = \"blue\"\n",
        "HIGH_QUALITY_COLOR = \"red\"\n",
        "\n",
        "\n",
        "ATTRIBUTES = [\n",
        "    'fixedAcidity',\n",
        "    'volatileAcidity',\n",
        "    'citricAcid',\n",
        "    'residualSugar',\n",
        "    'chlorides',\n",
        "    'freeSulfurDioxide',\n",
        "    'totalSulfurDioxide',\n",
        "    'density',\n",
        "    'pH',\n",
        "    'sulphates',\n",
        "    'alcohol',\n",
        "    'quality'\n",
        "    ]\n",
        "\n",
        "\n",
        "# label_indexes\n",
        "fixedAcidity = 0,\n",
        "volatileAcidity = 1,\n",
        "citricAcid = 2,\n",
        "residualSugar = 3\n",
        "chlorides = 4\n",
        "freeSulfurDioxide = 5\n",
        "totalSulfurDioxide = 6\n",
        "density = 7\n",
        "pH = 8\n",
        "sulphates = 9 \n",
        "alcohol = 10\n",
        "quality = 11\n",
        "\n",
        "\n",
        "LOW_QUALITY = 0\n",
        "HIGH_QUALITY = 1\n",
        "\n",
        "label_t = int\n",
        "\n",
        "\n",
        "\n",
        "####################################################################################################################################\n",
        "# Functions\n",
        "\n",
        "# use Euclidean distance to measure the similarity between instances\n",
        "def distance_euclidean(A : list[float], B : list[float]) -> float :\n",
        "    assert(len(A) == len(B))\n",
        "    return math.sqrt(sum([math.pow((t[0] - t[1]), 2) for t in zip(A, B) ]))\n",
        "\n",
        "assert(distance_euclidean([60], [42]) == 18.0)\n",
        "\n",
        "def isValidLabel(label) :\n",
        "  assert(\n",
        "    (label == HIGH_QUALITY) or \n",
        "    (label == LOW_QUALITY) \n",
        "  )\n",
        "\n",
        "def instance_label(instance) -> label_t :\n",
        "  #assert(len(instance) == len(ATTRIBUTES))\n",
        "  \n",
        "  label = instance[-1]\n",
        "  isValidLabel(label)\n",
        "  return label\n",
        "\n",
        "ACTUAL_LABELS = [ instance_label(test_instance) for test_instance in TESTING_DATA ]\n",
        "\n",
        "def instance_attributes(instance) :\n",
        "  #assert(len(instance) == len(ATTRIBUTES))\n",
        "  return instance[:-1]\n",
        "\n",
        "\n",
        "def attribute_distrib_summary(data) :\n",
        "\n",
        "  print()\n",
        "   \n",
        "  for i, attrib in enumerate(ATTRIBUTES) :\n",
        "    attrib_data = [row[i] for row in data]\n",
        "    # print(\"range   \", attrib, max(attrib_data) -min(attrib_data))\n",
        "\n",
        "    print(\"max     \", attrib, max(attrib_data))\n",
        "    print(\"average \", attrib, sum(attrib_data) / len(attrib_data))\n",
        "    print(\"min     \", attrib, min(attrib_data))\n",
        "    print()\n",
        "\n",
        "def data_label_distrib(data) :\n",
        "  print(Counter([instance_label(row) for row in data]))\n",
        "\n",
        "\n",
        "def serperateLabelsLowHigh(data) :\n",
        "  return [ \n",
        "   list(filter(lambda row : row[-1] == LOW_QUALITY, data)),\n",
        "   list(filter(lambda row : row[-1] == HIGH_QUALITY, data))\n",
        "  ]\n",
        "\n",
        "\n",
        "def get_column(rows, index) :\n",
        "  return [row[index] for row in rows]\n",
        "\n",
        "def generate_all_scatterplots(data, data_name : str) :\n",
        "\n",
        "  plt.cla()\n",
        "  plt.clf()\n",
        "\n",
        "  [low_rows, high_rows] = serperateLabelsLowHigh(data)\n",
        "\n",
        "  for attrib_x_index, attrib_x in enumerate(ATTRIBUTES) :\n",
        "    for attrib_y_index, attrib_y in enumerate(ATTRIBUTES) :\n",
        "\n",
        "\n",
        "\n",
        "      x_high = get_column(high_rows, attrib_x_index)\n",
        "      x_low = get_column(low_rows, attrib_x_index)\n",
        "\n",
        "      y_high = get_column(high_rows, attrib_y_index) \n",
        "      y_low =  get_column(low_rows, attrib_y_index) \n",
        "\n",
        "      # add in not high quality\n",
        "      plt.scatter(x_low, y_low, c=LOW_QUALITY_COLOR, alpha=0.3, label = \"Low quality\")\n",
        "      \n",
        "      # add in high quality\n",
        "      plt.scatter(x_high, y_high, c=HIGH_QUALITY_COLOR, alpha=0.3, label = \"High quality\")\n",
        "      \n",
        "      plt.xlabel(attrib_x)\n",
        "      plt.ylabel(attrib_y)\n",
        "\n",
        "      plotTitle = \"Scatter plot of \" + attrib_x + \" and \" + attrib_y\n",
        "      plt.title(plotTitle)\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "\n",
        "      plt.savefig(\"graphs/\"+data_name +\"/\"+plotTitle)\n",
        "\n",
        "      plt.cla()\n",
        "      plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "def generate_all_distributions(data, data_name : str) :\n",
        "\n",
        "  [low_rows, high_rows] = serperateLabelsLowHigh(data)\n",
        "\n",
        "  #assert(len(low_rows) != 0)\n",
        "  #assert(len(high_rows) != 0)\n",
        "  #assert(instance_label(low_rows[0]) == LOW_QUALITY)\n",
        "  #assert(instance_label(high_rows[0]) == HIGH_QUALITY)\n",
        "\n",
        "  for attrib_index, attrib in enumerate(ATTRIBUTES) :\n",
        "\n",
        "    high_values = get_column(high_rows, attrib_index)\n",
        "    low_values = get_column(low_rows, attrib_index)\n",
        "\n",
        "    # histogram looks weird\n",
        "    plt.cla()\n",
        "    plt.clf()\n",
        "\n",
        "    plt.xlabel(attrib)\n",
        "    plt.ylabel(\"frequency\")\n",
        "\n",
        "    plt.title(\"Histogram of \" + attrib)\n",
        "    plt.hist([high_values, low_values], bins=20, label=['high_values', 'low_values'], color=[HIGH_QUALITY_COLOR, LOW_QUALITY_COLOR])\n",
        "    plt.legend()\n",
        "    plt.savefig(\"hist_distributions/\"+data_name +\"/\"+attrib)\n",
        "\n",
        "    plt.cla()\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_predicted_hist(predicted_list : list[int], string) :\n",
        "\n",
        "\n",
        "  actual_list = [int(instance_label(instance)) for instance in TRAINING_DATA]\n",
        "  predicted_list = [int(label) for label in predicted_list]\n",
        "\n",
        "  plt.cla()\n",
        "  plt.clf()\n",
        "\n",
        "  plt.xlabel(\"quality\")\n",
        "  plt.ylabel(\"frequency\")\n",
        "\n",
        "  plt.title(\"Histogram of quality precicted by \" + string)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.hist([actual_list, predicted_list], bins=20, label=['actual', 'predicted'], color=[\"orange\", \"green\"])\n",
        "  plt.legend()\n",
        "  plt.savefig(\"predicted_hists/\"+string)\n",
        "\n",
        "\n",
        "\n",
        "  plt.cla()\n",
        "  plt.clf()\n",
        "\n",
        "\n",
        "def data_report(data) :\n",
        "  print(\"DATA REPORT START ========================\")\n",
        "  print()\n",
        "  data_label_distrib(data)\n",
        "  print()\n",
        "  attribute_distrib_summary(data)\n",
        "  print()\n",
        "  print(\"DATA REPORT END ========================\")\n",
        "\n",
        "import copy\n",
        "\n",
        "def predict_with_knn(test_instance : list[float], k : int, train_data) -> int :\n",
        "\n",
        "  assert(len(train_data) != 0)\n",
        "\n",
        "  test_instance_attributes = instance_attributes(test_instance)\n",
        "\n",
        "  instance_distance_to_label_array = [\n",
        "      (\n",
        "        distance_euclidean(instance_attributes(train_instance), test_instance_attributes),\n",
        "        instance_label(train_instance)\n",
        "      ) \n",
        "      for train_instance in train_data ]\n",
        "\n",
        "  \n",
        "\n",
        "  most_similar_train_instances = sorted(\n",
        "    instance_distance_to_label_array,\n",
        "    key = lambda x : x[0] # sort only by distance, not by label\n",
        "  )\n",
        "\n",
        "  # use majority vote to choose the label when K is greater than 1 \n",
        "  labels = [label for (distance, label) in most_similar_train_instances]\n",
        "  labels_of_k_closest = labels[:k]\n",
        "  counter = Counter(labels_of_k_closest)\n",
        "  max_frequency = max(counter.values())\n",
        "  most_frequent = list(filter(lambda keyValueTuple : keyValueTuple[1] == max_frequency, list(counter.items())))\n",
        "  label = most_frequent[0][0]\n",
        "  isValidLabel(label)\n",
        "\n",
        "  # if majority vote results in a tie, tie break by taking the label of the 1-NN \n",
        "  if len(most_frequent) > 1 :\n",
        "\n",
        "    # if there is a tie due to 2 or more instances having exactly the same distance, tie break by choosing randomly among these\n",
        "    closest_distance = max([dist for (dist, label) in instance_distance_to_label_array])\n",
        "    closest = list(filter(lambda keyValueTuple : keyValueTuple[0] == closest_distance, instance_distance_to_label_array))\n",
        "    random.shuffle(closest)\n",
        "    return closest[0][1]\n",
        "  \n",
        "  return label\n",
        "\n",
        "\n",
        "def check_accuracy(predict_instance_label : Callable[[list[float]], int], testing_data , prediction_name : str) :\n",
        "\n",
        "  # test each instance in the test data\n",
        "  predicted = [ predict_instance_label(test_instance) for test_instance in testing_data ]\n",
        "  actual =  [ instance_label(test_instance) for test_instance in testing_data ]\n",
        "\n",
        "  conf = confusion_matrix(ACTUAL_LABELS, predicted)\n",
        "\n",
        "  generate_predicted_hist(predicted, prediction_name)\n",
        "\n",
        "\n",
        "  accur = sum(conf[i][i] for i in range(len(conf))) / sum(sum(x) for x in conf)\n",
        "  print(\"ACCURACY : \" + str(accur) + \" \" + prediction_name)\n",
        "  print(\"     Class 0 : \", str(Counter(predicted)[0] / len(predicted))[:5])\n",
        "  print(\"     Class 1 : \", str(Counter(predicted)[1] / len(predicted))[:5])\n",
        "  print(\"     bias to class 1 \", str((Counter(predicted)[1] / len(predicted)) - (Counter(actual)[1] / len(predicted)))[:5])\n",
        "\n",
        "  # https://stackoverflow.com/questions/35572000/how-can-i-plot-a-confusion-matrix\n",
        "\n",
        "\n",
        "  plt.cla()\n",
        "  plt.clf() # TODO avoid this\n",
        "\n",
        "\n",
        "  plt.figure(figsize = (10,7))\n",
        "\n",
        "  # on the side is actual\n",
        "  # on the bottom is predicted\n",
        "\n",
        "  sn.heatmap(conf, annot=True, fmt=\".0f\")\n",
        "\n",
        "  plt.xlabel(\"predicted\")\n",
        "  plt.ylabel(\"actual\")\n",
        "\n",
        "  plt.title(\"Confusion matrix for \" + prediction_name)\n",
        "  plt.savefig(\"confusion/\" + prediction_name)\n",
        "\n",
        "  plt.cla()\n",
        "  plt.clf()\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "  # saying positive is high quality\n",
        "  # false = 0, true = 1, negative = 0, positive = 0\n",
        "  # [[falseNegative, falsePositive],\n",
        "  #  [trueNegative,   truePositive]]\n",
        "\n",
        "  trueFalseNegativePositive = [[[],[]],[[],[]]]\n",
        "  for i, inst in enumerate(testing_data):\n",
        "\n",
        "    isTrue = int(instance_label(inst) == predicted[i])\n",
        "    isPositive = int(instance_label(inst))\n",
        "    trueFalseNegativePositive[isTrue][isPositive].append(inst)\n",
        "\n",
        "\n",
        "  flattened_confusion_columns = [trueFalseNegativePositive[0][0], trueFalseNegativePositive[0][1], trueFalseNegativePositive[1][0], trueFalseNegativePositive[1][1]] # TODO get order right\n",
        "  flattened_confusion_columns_labels = [\"falseNegative\", \"falsePositive\", \"trueNegative\", \"TruePositive\"]\n",
        "  \n",
        "  # create all possible combinations of attributes\n",
        "  for attrib_x_index, attrib_x in enumerate(ATTRIBUTES) :\n",
        "    for attrib_y_index, attrib_y in enumerate(ATTRIBUTES) :\n",
        "\n",
        "      for columns, colour, label in zip(flattened_confusion_columns, [\"green\", \"red\", \"blue\", \"black\"], flattened_confusion_columns_labels) : # TODO add labels\n",
        "\n",
        "        xs = get_column(columns, attrib_x_index)\n",
        "        ys =  get_column(columns, attrib_y_index) \n",
        "\n",
        "        plt.scatter(xs, ys, c=colour, alpha=0.3, label=f\"Class {label}\")\n",
        "\n",
        "      plt.xlabel(attrib_x)\n",
        "      plt.ylabel(attrib_y)\n",
        "\n",
        "      plotTitle = \"confusion_scatters/\"+prediction_name+\"/\"+attrib_x+\"_\"+attrib_y\n",
        "      plt.title(plotTitle)\n",
        "      plt.legend()\n",
        "      plt.grid(True)\n",
        "\n",
        "      plt.savefig(plotTitle)\n",
        "\n",
        "      plt.cla()\n",
        "      plt.clf()\n",
        "\n",
        "\n",
        "  return predicted\n",
        "\n",
        "\n",
        "# FROM SPEC\n",
        "\"\"\"\n",
        "Note that you should use the training dataset to get the parameters for these normalizations, and apply the same parameters to both the training and test instances (e.g., min(x) refers to the minimum value of attribute x in the training dataset; use this same minimum value when scaling attribute x in the test set).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# create a scale function for each column\n",
        "\n",
        "def min_max_scale_from_training(train_column : list[float]) :\n",
        "  max_train = max(train_column)\n",
        "  min_train = min(train_column)\n",
        "  range_train = max_train - min_train\n",
        "\n",
        "  def min_max_scale(ls : list[float]) :\n",
        "    return [(x - min_train) / range_train for x in ls]\n",
        "\n",
        "  return min_max_scale\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mean(ls : list[float]) : \n",
        "  return sum(ls) / len(ls)\n",
        "\n",
        "\n",
        "def variance(ls : list[float]) : \n",
        "  m = mean(ls)\n",
        "  return sum([math.pow((x - m), 2) / len(ls) for x in ls])\n",
        "\n",
        "\n",
        "def standardDeviation(ls : list[float]) :\n",
        "  return math.sqrt(variance(ls))\n",
        "\n",
        "\n",
        "def distribution_scale_from_trainging(train_column  : list[float]) :\n",
        "\n",
        "  mean_train = mean(train_column)\n",
        "  stddev_train = standardDeviation(train_column)\n",
        "\n",
        "  def distribution_scale(ls : list[float]) :\n",
        "    return [(x - mean_train) / stddev_train for x in ls]\n",
        "  \n",
        "  return distribution_scale\n",
        "\n",
        "# \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getColum(arr, c : int) :\n",
        "  return [ row[c] for row in arr ]\n",
        "\n",
        "def flip(arr : list[list[float]]) :\n",
        "\n",
        "  return [getColum(arr, c) for c in range(len(arr[1]))]\n",
        "\n",
        "def scaleColumns(training_data, current_data : list[list[float]], f_from_training_data) :\n",
        "  \n",
        "  \n",
        "  # flip to map columns rather than rows\n",
        "\n",
        "  current_data = flip(current_data)\n",
        "  training_data = flip(training_data)\n",
        "  \n",
        "\n",
        "  for i, column in enumerate(current_data[:-1]) :\n",
        "    \n",
        "    f = f_from_training_data(training_data[i])\n",
        "    current_data[i] = f(column)\n",
        "\n",
        "\n",
        "  current_data = flip(current_data)\n",
        "  training_data = flip(training_data)\n",
        "\n",
        "  return current_data\n",
        "\n",
        "\n",
        "\n",
        "# visually see if knn is working with a 2d scatter plot\n",
        "def visual_knn_test() :\n",
        "\n",
        "  plotTitle = \"visual_knn_test\"\n",
        "  plt.title(plotTitle)\n",
        "  plt.grid(True)\n",
        "\n",
        "  testing_data_list = [\n",
        "    np.random.uniform(low=-2, high=2, size=(500, 3)),\n",
        "    #[[0,0.5, None], [1,0.5, None], [1,1, None], [-1,-1,None]] # tests that the test labels aren't being used\n",
        "  ]\n",
        "  \n",
        "  training_data_list = [\n",
        "    [[-1,-1,0],[1,1,0], [0,0,1]],\n",
        "    [[-1,1,0],[1,1,0], [0,1,1]],\n",
        "    [[1,1,0],[-1,-1,0], [-1,1,1], [1,-1,1]],\n",
        "    [[-1,0,0],[1,0,0], [0,1,1], [0,1,1]]\n",
        "  ]\n",
        "\n",
        "  for k in [1,2] : \n",
        "    for train_i, training_data in enumerate(training_data_list) :\n",
        "      for test_i, testing_data in enumerate(testing_data_list) :\n",
        "\n",
        "        for instance in training_data :\n",
        "          label = instance_label(instance)\n",
        "          if (label == 0) :\n",
        "            colour = \"green\"\n",
        "          else : \n",
        "            colour = \"black\"\n",
        "          plt.scatter(instance[0], instance[1], c=colour, alpha=1)\n",
        "      \n",
        "\n",
        "        for instance in testing_data :\n",
        "          label = predict_with_knn(instance, k, training_data)\n",
        "          if (label == 0) :\n",
        "            colour = \"red\"\n",
        "          elif (label == 1) :\n",
        "            colour = \"blue\"\n",
        "          \n",
        "\n",
        "          plt.scatter(instance[0], instance[1], c=colour, alpha=0.3)\n",
        "\n",
        "        plt.savefig(plotTitle + str(k) + str(train_i) + \"_\" + str(test_i))\n",
        "        plt.cla()\n",
        "        plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntOpzhc75SKu"
      },
      "source": [
        "## 1. K-NN classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osKu2XOP5SKu",
        "outputId": "328583ce-f4b6-4b9e-8e46-f3f8e6da2563"
      },
      "outputs": [],
      "source": [
        "# see predict_with_knn function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOmoLT4d5SKu"
      },
      "source": [
        "## 2. 1-NN classification\n",
        "\n",
        "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MOu3TrQH5SKu"
      },
      "outputs": [],
      "source": [
        "knn_predicted = check_accuracy(lambda instnace : predict_with_knn(instnace, 1, TRAINING_DATA), TESTING_DATA, \"knn\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBXeyLOB5SKu"
      },
      "source": [
        "## 3. Normalization\n",
        "\n",
        "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3GyssAU5SKu"
      },
      "outputs": [],
      "source": [
        "# scale columns\n",
        "min_max_scaled_training_data = scaleColumns(TRAINING_DATA, TRAINING_DATA, min_max_scale_from_training)\n",
        "min_max_scaled_test_data = scaleColumns(TRAINING_DATA, TESTING_DATA, min_max_scale_from_training)\n",
        "\n",
        "distribution_scaled_training_data = scaleColumns(TRAINING_DATA, TRAINING_DATA, distribution_scale_from_trainging)\n",
        "distribution_scaled_test_data = scaleColumns(TRAINING_DATA, TESTING_DATA, distribution_scale_from_trainging)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pebQI61j5SKv"
      },
      "source": [
        "## 4. Model extensions\n",
        "\n",
        "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhAs_csF5SKv"
      },
      "source": [
        "### 4.1\n",
        "Compare the performance of your best 1-NN model from Question 3 to a Gaussian naive Bayes model on this dataset (you may use library functions to implement the Gaussian naive Bayes model). In your write-up, state the accuracy of the naive Bayes model and identify instances where the two models disagree. Why do the two models classify these instances differently?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Kmy3hxEe5SKv"
      },
      "outputs": [],
      "source": [
        "#NA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8ML4Hv85SKv"
      },
      "source": [
        "### 4.2\n",
        "Implement two additional distance measures for your K-NN model: cosine similarity and Mahalanobis distance (you may use library functions for these distance measures). Do 1-NN classification using each of these new distance measures and the three normalization options from Question 3. Discuss how the new distance metrics compare to Euclidean distance and how each metric is affected by normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sKjB1O6n5SKv"
      },
      "outputs": [],
      "source": [
        "#NA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGF1_jYb5SKv"
      },
      "source": [
        "### 4.3\n",
        "Implement either of the two K-NN weighting strategies discussed in lecture (inverse linear distance or inverse distance). Compare the performance of the weighted and majority vote models for a few different values of K. In your write-up, discuss how weighting strategy and the value of K affect the model's decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J8ljCODQ5SKv"
      },
      "outputs": [],
      "source": [
        "#NA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGcHI6t_5SKv"
      },
      "source": [
        "### 4.4\n",
        "Measure the empirical distribution of class labels in the training dataset (what percentage of the training data comes from each class). Then evaluate the distribution of labels predicted by your K-NN model for the test data, for a range of values for K. Does the class distribution of the predicted labels match the class distribution of the training data? Explain why or why not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "y2jIF4po5SKv"
      },
      "outputs": [],
      "source": [
        "for k in range(1,6) :\n",
        "\n",
        "  print(\"k = \", k)\n",
        "  knn_predicted = check_accuracy(lambda instnace : predict_with_knn(instnace, k, TRAINING_DATA), TESTING_DATA, \"knn\") \n",
        "  knn_with_min_max_normalisation_predicted = check_accuracy(lambda instnace : predict_with_knn(instnace, k, min_max_scaled_training_data), min_max_scaled_test_data, \"knn_with_min_max_normalisation\") \n",
        "  knn_with_distribution_normalisation_predicted = check_accuracy(lambda instnace : predict_with_knn(instnace, k, distribution_scaled_training_data), distribution_scaled_test_data, \"knn_with_distribution_normalisation\") \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
